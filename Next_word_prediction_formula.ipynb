{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpJzgf7Eo/S2Qx0hwba/Kc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dineshkumar-2003/Datascience-projects/blob/main/Next_word_prediction_formula.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgkGZ2euTaI_",
        "outputId": "3a3424fd-5452-422a-99a6-4754da21e817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab: ['<pad>', '<unk>', 'dosa', 'eat', 'he', 'i', 'idly', 'like', 'pizza', 'pongal', 'runs', 'sushi', 'to']\n",
            "Epoch 0, Loss: 30.7654\n",
            "Epoch 100, Loss: 17.4240\n",
            "Epoch 200, Loss: 15.7363\n",
            "Epoch 300, Loss: 14.6583\n",
            "Epoch 400, Loss: 13.8757\n",
            "Generated: i like to pizza dosa pongal eat eat\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# --- Dataset ---\n",
        "sentences = [\n",
        "    \"i like to eat pizza\",\n",
        "    \"i like to eat sushi\",\n",
        "    \"he runs to eat pizza\",\n",
        "    \"i like to eat dosa\",\n",
        "    \"i like to eat pongal\",\n",
        "    \"i like to eat idly\",\n",
        "]\n",
        "\n",
        "\n",
        "tokens = [word.lower() for s in sentences for word in s.split()]\n",
        "counts = Counter(tokens)\n",
        "\n",
        "# Add special tokens\n",
        "vocab = [\"<pad>\", \"<unk>\"] + sorted(set(counts))\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(\"Vocab:\", vocab)\n",
        "\n",
        "# --- Config ---\n",
        "#vocab = [\"<pad>\", \"<unk>\", \"i\", \"like\", \"to\", \"eat\", \"pizza\", \"sushi\", \"he\", \"runs\"]\n",
        "vocab_size = len(vocab)\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "embedding_dim = 8\n",
        "seq_len = 3\n",
        "num_heads = 2\n",
        "hidden_dim = 16\n",
        "learning_rate = 0.01\n",
        "epochs = 500\n",
        "\n",
        "# --- Tokenizer ---\n",
        "def tokenize(sentence):\n",
        "    return [word_to_idx.get(word.lower(), word_to_idx[\"<unk>\"]) for word in sentence.split()]\n",
        "\n",
        "\n",
        "data = []\n",
        "for s in sentences:\n",
        "    tokens = tokenize(s)\n",
        "    for i in range(len(tokens) - seq_len):\n",
        "        context = tokens[i:i + seq_len]\n",
        "        next_word = tokens[i + seq_len]\n",
        "        data.append((context, next_word))\n",
        "\n",
        "# --- Positional Encoding ---\n",
        "def get_positional_encoding(seq_len, d_model):\n",
        "    PE = np.zeros((seq_len, d_model))\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            PE[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "            if i+1 < d_model:\n",
        "                PE[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i)/d_model)))\n",
        "    return PE\n",
        "\n",
        "# --- Attention ---\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = Q @ K.T / np.sqrt(d_k)  # (seq_len, seq_len)\n",
        "    attn = softmax(scores)\n",
        "    return attn @ V  # (seq_len, depth)\n",
        "\n",
        "# --- Multi-head Attention ---\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = d_model // num_heads\n",
        "        self.Wq = [np.random.randn(d_model, self.depth) * 0.1 for _ in range(num_heads)]\n",
        "        self.Wk = [np.random.randn(d_model, self.depth) * 0.1 for _ in range(num_heads)]\n",
        "        self.Wv = [np.random.randn(d_model, self.depth) * 0.1 for _ in range(num_heads)]\n",
        "        self.Wo = np.random.randn(d_model, d_model) * 0.1\n",
        "\n",
        "    def forward(self, x):\n",
        "        heads = []\n",
        "        for i in range(self.num_heads):\n",
        "            Q = x @ self.Wq[i]\n",
        "            K = x @ self.Wk[i]\n",
        "            V = x @ self.Wv[i]\n",
        "            head = scaled_dot_product_attention(Q, K, V)\n",
        "            heads.append(head)\n",
        "        concat = np.concatenate(heads, axis=-1)\n",
        "        return concat @ self.Wo\n",
        "\n",
        "# --- Feed Forward ---\n",
        "class FeedForward:\n",
        "    def __init__(self, d_model, hidden_dim):\n",
        "        self.W1 = np.random.randn(d_model, hidden_dim) * 0.1\n",
        "        self.b1 = np.zeros((hidden_dim,))\n",
        "        self.W2 = np.random.randn(hidden_dim, d_model) * 0.1\n",
        "        self.b2 = np.zeros((d_model,))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return np.maximum(0, x @ self.W1 + self.b1) @ self.W2 + self.b2\n",
        "\n",
        "# --- Transformer Block ---\n",
        "class TransformerBlock:\n",
        "    def __init__(self, d_model, num_heads, hidden_dim):\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha.forward(x)\n",
        "        x = x + self.ffn.forward(x)\n",
        "        return x\n",
        "\n",
        "# --- Simple Transformer ---\n",
        "class SimpleTransformer:\n",
        "    def __init__(self, vocab_size, d_model, seq_len, num_heads, hidden_dim):\n",
        "        self.embeddings = np.random.randn(vocab_size, d_model) * 0.1\n",
        "        self.positional_encoding = get_positional_encoding(seq_len, d_model)\n",
        "        self.block = TransformerBlock(d_model, num_heads, hidden_dim)\n",
        "        self.output_layer = np.random.randn(d_model, vocab_size) * 0.1\n",
        "\n",
        "    def forward(self, x_ids):\n",
        "        x = self.embeddings[x_ids] + self.positional_encoding\n",
        "        x = self.block.forward(x)\n",
        "        final_token = x[-1]\n",
        "        logits = final_token @ self.output_layer\n",
        "        return softmax(logits)\n",
        "\n",
        "# --- Training ---\n",
        "model = SimpleTransformer(vocab_size, embedding_dim, seq_len, num_heads, hidden_dim)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for context, target in data:\n",
        "        probs = model.forward(context)\n",
        "        target_one_hot = np.zeros(vocab_size)\n",
        "        target_one_hot[target] = 1\n",
        "        loss = -np.log(probs[target] + 1e-9)\n",
        "        total_loss += loss\n",
        "\n",
        "        # Gradient descent (manual)\n",
        "        grad_output = probs.copy()\n",
        "        grad_output[target] -= 1\n",
        "        grad = np.outer(model.block.forward(model.embeddings[context] + model.positional_encoding)[-1], grad_output)\n",
        "        model.output_layer -= learning_rate * grad\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# # --- Prediction Function ---\n",
        "# def predict_next_word(context_words):\n",
        "#     x_ids = tokenize(\" \".join(context_words))\n",
        "#     probs = model.forward(x_ids)\n",
        "#     return idx_to_word[np.argmax(probs)]\n",
        "\n",
        "# # Predict\n",
        "# predict_next_word([\"i\", \"like\", \"to\", \"eat\"])\n",
        "\n",
        "def sample_with_temperature(probs, temperature=1.0):\n",
        "    if temperature == 0:\n",
        "        return np.argmax(probs)\n",
        "    scaled = np.log(probs + 1e-9) / temperature\n",
        "    scaled = np.exp(scaled) / np.sum(np.exp(scaled))\n",
        "    return np.random.choice(len(probs), p=scaled)\n",
        "\n",
        "\n",
        "\n",
        "def generate_sequence(model, context_words, num_words):\n",
        "    generated = context_words.copy()\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        # Trim or pad to seq_len\n",
        "        input_seq = generated[-seq_len:]\n",
        "        if len(input_seq) < seq_len:\n",
        "            padding = [\"<pad>\"] * (seq_len - len(input_seq))\n",
        "            input_seq = padding + input_seq\n",
        "\n",
        "        x_ids = tokenize(\" \".join(input_seq))\n",
        "        probs = model.forward(x_ids)\n",
        "        next_word_idx = sample_with_temperature(probs, temperature=1.0)  # 0.7–1.0 is usually nice\n",
        "        next_word = idx_to_word[next_word_idx]\n",
        "        generated.append(next_word)\n",
        "\n",
        "    return \" \".join(generated)\n",
        "\n",
        "\n",
        "generated_text = generate_sequence(model, [\"i\", \"like\", \"to\"], num_words=5)\n",
        "print(\"Generated:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d2lUWj5tTtL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# --- Dataset ---\n",
        "sentences = [\n",
        "    \"i like to eat pizza\",\n",
        "    \"i like to eat sushi\",\n",
        "    \"he runs to eat pizza\",\n",
        "    \"i like to eat dosa\",\n",
        "    \"i like to eat pongal\",\n",
        "    \"i like to eat idly\",\n",
        "]\n",
        "\n",
        "# --- Vocabulary Building ---\n",
        "tokens = [word.lower() for s in sentences for word in s.split()] + [\"<eos>\"]\n",
        "counts = Counter(tokens)\n",
        "vocab = [\"<pad>\", \"<unk>\"] + sorted(set(counts))\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# --- Config ---\n",
        "embedding_dim = 8\n",
        "seq_len = 3  # Now using 3-word context\n",
        "num_heads = 2\n",
        "hidden_dim = 16\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# --- Tokenizer ---\n",
        "def tokenize(sentence):\n",
        "    return [word_to_idx.get(word.lower(), word_to_idx[\"<unk>\"]) for word in sentence.split()]\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "data = []\n",
        "for s in sentences:\n",
        "    tokens = tokenize(s) + [word_to_idx[\"<eos>\"]]\n",
        "    for i in range(len(tokens) - seq_len):\n",
        "        context = tokens[i:i + seq_len]\n",
        "        next_word = tokens[i + seq_len]\n",
        "        data.append((context, next_word))\n",
        "\n",
        "# --- Positional Encoding ---\n",
        "def get_positional_encoding(seq_len, d_model):\n",
        "    PE = np.zeros((seq_len, d_model))\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            PE[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "            if i+1 < d_model:\n",
        "                PE[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i)/d_model)))\n",
        "    return PE\n",
        "\n",
        "# --- Attention Components ---\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = Q @ K.T / np.sqrt(d_k)\n",
        "    attn = softmax(scores)\n",
        "    return attn @ V\n",
        "\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = d_model // num_heads\n",
        "        self.Wq = [np.random.randn(d_model, self.depth) * 0.1 for _ in range(num_heads)]\n",
        "        self.Wk = [np.random.randn(d_model, self.depth) * 0.1 for _ in range(num_heads)]\n",
        "        self.Wv = [np.random.randn(d_model, self.depth) * 0.1 for _ in range(num_heads)]\n",
        "        self.Wo = np.random.randn(d_model, d_model) * 0.1\n",
        "\n",
        "    def forward(self, x):\n",
        "        heads = []\n",
        "        for i in range(self.num_heads):\n",
        "            Q = x @ self.Wq[i]\n",
        "            K = x @ self.Wk[i]\n",
        "            V = x @ self.Wv[i]\n",
        "            head = scaled_dot_product_attention(Q, K, V)\n",
        "            heads.append(head)\n",
        "        concat = np.concatenate(heads, axis=-1)\n",
        "        return concat @ self.Wo\n",
        "\n",
        "class FeedForward:\n",
        "    def __init__(self, d_model, hidden_dim):\n",
        "        self.W1 = np.random.randn(d_model, hidden_dim) * 0.1\n",
        "        self.b1 = np.zeros((hidden_dim,))\n",
        "        self.W2 = np.random.randn(hidden_dim, d_model) * 0.1\n",
        "        self.b2 = np.zeros((d_model,))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return np.maximum(0, x @ self.W1 + self.b1) @ self.W2 + self.b2\n",
        "\n",
        "class TransformerBlock:\n",
        "    def __init__(self, d_model, num_heads, hidden_dim):\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha.forward(x)\n",
        "        x = x + self.ffn.forward(x)\n",
        "        return x\n",
        "\n",
        "class SimpleTransformer:\n",
        "    def __init__(self, vocab_size, d_model, seq_len, num_heads, hidden_dim):\n",
        "        self.embeddings = np.random.randn(vocab_size, d_model) * 0.1\n",
        "        self.positional_encoding = get_positional_encoding(seq_len, d_model)\n",
        "        self.block = TransformerBlock(d_model, num_heads, hidden_dim)\n",
        "        self.output_layer = np.random.randn(d_model, vocab_size) * 0.1\n",
        "\n",
        "    def forward(self, x_ids):\n",
        "        x = self.embeddings[x_ids] + self.positional_encoding\n",
        "        x = self.block.forward(x)\n",
        "        final_token = x[-1]  # Use last token\n",
        "        logits = final_token @ self.output_layer\n",
        "        return softmax(logits), final_token\n",
        "\n",
        "# --- Training ---\n",
        "model = SimpleTransformer(vocab_size, embedding_dim, seq_len, num_heads, hidden_dim)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for context, target in data:\n",
        "        probs, last_token_embedding = model.forward(context)\n",
        "        target_one_hot = np.zeros(vocab_size)\n",
        "        target_one_hot[target] = 1\n",
        "        loss = -np.log(probs[target] + 1e-9)\n",
        "        total_loss += loss\n",
        "\n",
        "        grad_output = probs.copy()\n",
        "        grad_output[target] -= 1\n",
        "        grad = np.outer(last_token_embedding, grad_output)\n",
        "\n",
        "        # Update output layer\n",
        "        model.output_layer -= learning_rate * grad\n",
        "\n",
        "        # Update embeddings\n",
        "        for i, word_id in enumerate(context):\n",
        "            model.embeddings[word_id] -= learning_rate * grad_output @ model.output_layer.T\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# --- Prediction Utilities ---\n",
        "def sample_with_temperature(probs, temperature=1.0):\n",
        "    if temperature == 0:\n",
        "        return np.argmax(probs)\n",
        "    scaled = np.log(probs + 1e-9) / temperature\n",
        "    scaled = np.exp(scaled) / np.sum(np.exp(scaled))\n",
        "    return np.random.choice(len(probs), p=scaled)\n",
        "\n",
        "def generate_sequence(model, context_words, num_words):\n",
        "    generated = context_words.copy()\n",
        "    for _ in range(num_words):\n",
        "        input_seq = generated[-seq_len:]\n",
        "        if len(input_seq) < seq_len:\n",
        "            padding = [\"<pad>\"] * (seq_len - len(input_seq))\n",
        "            input_seq = padding + input_seq\n",
        "        x_ids = tokenize(\" \".join(input_seq))\n",
        "        probs, _ = model.forward(x_ids)\n",
        "        next_word_idx = sample_with_temperature(probs, temperature=0.8)\n",
        "        next_word = idx_to_word[next_word_idx]\n",
        "        if next_word == \"<eos>\":\n",
        "            break\n",
        "        generated.append(next_word)\n",
        "    return \" \".join(generated)\n",
        "\n",
        "def predict_next_word(context_words):\n",
        "    x_ids = tokenize(\" \".join(context_words))\n",
        "    probs, _ = model.forward(x_ids)\n",
        "    return idx_to_word[np.argmax(probs)]\n",
        "\n",
        "# --- Example Predictions ---\n",
        "print(\"Next word after 'i like to':\", predict_next_word([\"i\", \"like\", \"to\"]))\n",
        "print(\"Generated sequence:\", generate_sequence(model, [\"i\", \"like\", \"to\"], num_words=5))\n",
        "print(\"Generated sequence:\", generate_sequence(model, [\"hr\", \"runs\"], num_words=5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty1wWY7K3VrM",
        "outputId": "2030d289-f3d3-4d4a-84c6-be6034115d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 47.8853\n",
            "Epoch 200, Loss: 14.9743\n",
            "Epoch 400, Loss: 13.8410\n",
            "Epoch 600, Loss: 15.3992\n",
            "Epoch 800, Loss: 18.4507\n",
            "Next word after 'i like to': eat\n",
            "Generated sequence: i like to eat idly\n",
            "Generated sequence: hr runs eat idly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GQrlPDgR3WFq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}